<!DOCTYPE html>
<html lang="zh-CN">
<head>
  <meta charset="UTF-8">
<meta name="viewport" content="width=device-width">
<meta name="theme-color" content="#335599"><meta name="generator" content="Hexo 5.4.2">


  <link rel="apple-touch-icon" sizes="180x180" href="/images/apple-touch-icon-cyclinbox.png">
  <link rel="icon" type="image/png" sizes="32x32" href="/images/favicon-32x32-cyclinbox.png">
  <link rel="icon" type="image/png" sizes="16x16" href="/images/favicon-16x16-cyclinbox.png">
  <link rel="mask-icon" href="/images/apple-touch-icon-cyclinbox.png" color="#335599">

<link rel="stylesheet" href="/css/main.css">



<link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.2.0/css/all.min.css" integrity="sha256-AbA177XfpSnFEvgpYu1jMygiLabzPCJCRIBtR5jGc0k=" crossorigin="anonymous">
  <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/pace/1.2.4/themes/blue/pace-theme-minimal.css">
  <script src="https://cdnjs.cloudflare.com/ajax/libs/pace/1.2.4/pace.min.js" integrity="sha256-gqd7YTjg/BtfqWSwsJOvndl0Bxc8gFImLEkXQT8+qj0=" crossorigin="anonymous"></script>

<script class="next-config" data-name="main" type="application/json">{"hostname":"wz.anoms.top","root":"/","images":"/images","scheme":"Gemini","darkmode":false,"version":"8.13.0","exturl":false,"sidebar":{"position":"left","display":"always","padding":18,"offset":12},"copycode":{"enable":true,"style":"flat"},"bookmark":{"enable":false,"color":"#37c0c6","save":"auto"},"mediumzoom":false,"lazyload":false,"pangu":false,"comments":{"style":"tabs","active":null,"storage":true,"lazyload":false,"nav":null},"stickytabs":true,"motion":{"enable":false,"async":false,"transition":{"post_block":"fadeIn","post_header":"fadeInDown","post_body":"fadeInDown","coll_header":"fadeInLeft","sidebar":"fadeInUp"}},"prism":false,"i18n":{"placeholder":"搜索...","empty":"没有找到任何搜索结果：${query}","hits_time":"找到 ${hits} 个搜索结果（用时 ${time} 毫秒）","hits":"找到 ${hits} 个搜索结果"},"path":"/search.xml","localsearch":{"enable":true,"trigger":"auto","top_n_per_article":1,"unescape":false,"preload":false}}</script><script src="/js/config.js"></script>

    <meta name="description" content="时隔半年，再次复习一下attention机制，并学习一下什么是self-attention。">
<meta property="og:type" content="article">
<meta property="og:title" content="Attention机制与self-attention详解（二）——Transformer架构">
<meta property="og:url" content="https://wz.anoms.top/2023/12/09/attention_architecture_2/index.html">
<meta property="og:site_name" content="Warren&#39;s Blog">
<meta property="og:description" content="时隔半年，再次复习一下attention机制，并学习一下什么是self-attention。">
<meta property="og:locale" content="zh_CN">
<meta property="og:image" content="https://wzblog-1311469384.cos.ap-shanghai.myqcloud.com/wzblog/20231209131238.png">
<meta property="og:image" content="https://wzblog-1311469384.cos.ap-shanghai.myqcloud.com/wzblog/20231209131916.png">
<meta property="og:image" content="https://wzblog-1311469384.cos.ap-shanghai.myqcloud.com/wzblog/20231209132152.png">
<meta property="og:image" content="https://wzblog-1311469384.cos.ap-shanghai.myqcloud.com/wzblog/20231209132546.png">
<meta property="og:image" content="https://wzblog-1311469384.cos.ap-shanghai.myqcloud.com/wzblog/20231209132932.png">
<meta property="og:image" content="https://wzblog-1311469384.cos.ap-shanghai.myqcloud.com/wzblog/20231209133005.png">
<meta property="og:image" content="https://wzblog-1311469384.cos.ap-shanghai.myqcloud.com/wzblog/20231209133108.png">
<meta property="og:image" content="https://wzblog-1311469384.cos.ap-shanghai.myqcloud.com/wzblog/20231209133246.png">
<meta property="og:image" content="https://wzblog-1311469384.cos.ap-shanghai.myqcloud.com/wzblog/20231209133352.png">
<meta property="og:image" content="https://wzblog-1311469384.cos.ap-shanghai.myqcloud.com/wzblog/20231209133536.png">
<meta property="og:image" content="https://wzblog-1311469384.cos.ap-shanghai.myqcloud.com/wzblog/20231209134025.png">
<meta property="og:image" content="https://wzblog-1311469384.cos.ap-shanghai.myqcloud.com/wzblog/20231209134443.png">
<meta property="og:image" content="https://wzblog-1311469384.cos.ap-shanghai.myqcloud.com/wzblog/20231209134649.png">
<meta property="og:image" content="https://wzblog-1311469384.cos.ap-shanghai.myqcloud.com/wzblog/20231209232823.png">
<meta property="og:image" content="https://wzblog-1311469384.cos.ap-shanghai.myqcloud.com/wzblog/20231209233212.png">
<meta property="og:image" content="https://wzblog-1311469384.cos.ap-shanghai.myqcloud.com/wzblog/20231209234236.png">
<meta property="og:image" content="https://wzblog-1311469384.cos.ap-shanghai.myqcloud.com/wzblog/20231209234436.png">
<meta property="og:image" content="https://wzblog-1311469384.cos.ap-shanghai.myqcloud.com/wzblog/20231209234529.png">
<meta property="og:image" content="https://wzblog-1311469384.cos.ap-shanghai.myqcloud.com/wzblog/20231209235154.png">
<meta property="og:image" content="https://wzblog-1311469384.cos.ap-shanghai.myqcloud.com/wzblog/20231209235637.png">
<meta property="og:image" content="https://wzblog-1311469384.cos.ap-shanghai.myqcloud.com/wzblog/20231209235809.png">
<meta property="og:image" content="https://wzblog-1311469384.cos.ap-shanghai.myqcloud.com/wzblog/20231210000746.png">
<meta property="og:image" content="https://wzblog-1311469384.cos.ap-shanghai.myqcloud.com/wzblog/20231210001954.png">
<meta property="og:image" content="https://wzblog-1311469384.cos.ap-shanghai.myqcloud.com/wzblog/20231210003140.png">
<meta property="og:image" content="https://wzblog-1311469384.cos.ap-shanghai.myqcloud.com/wzblog/20231210003505.png">
<meta property="og:image" content="https://wzblog-1311469384.cos.ap-shanghai.myqcloud.com/wzblog/20231210003545.png">
<meta property="og:image" content="https://wzblog-1311469384.cos.ap-shanghai.myqcloud.com/wzblog/20231210003903.png">
<meta property="og:image" content="https://wzblog-1311469384.cos.ap-shanghai.myqcloud.com/wzblog/20231210004139.png">
<meta property="og:image" content="https://wzblog-1311469384.cos.ap-shanghai.myqcloud.com/wzblog/20231210005034.png">
<meta property="og:image" content="https://wzblog-1311469384.cos.ap-shanghai.myqcloud.com/wzblog/20231210005433.png">
<meta property="og:image" content="https://wzblog-1311469384.cos.ap-shanghai.myqcloud.com/wzblog/20231210005808.png">
<meta property="og:image" content="https://wzblog-1311469384.cos.ap-shanghai.myqcloud.com/wzblog/20231210010238.png">
<meta property="og:image" content="https://wzblog-1311469384.cos.ap-shanghai.myqcloud.com/wzblog/20231210010740.png">
<meta property="article:published_time" content="2023-12-09T13:38:09.000Z">
<meta property="article:modified_time" content="2023-12-10T15:13:39.059Z">
<meta property="article:author" content="Warren Z">
<meta property="article:tag" content="计算机技术杂谈">
<meta property="article:tag" content="Attention机制">
<meta property="article:tag" content="机器学习">
<meta property="article:tag" content="Transformer">
<meta name="twitter:card" content="summary">
<meta name="twitter:image" content="https://wzblog-1311469384.cos.ap-shanghai.myqcloud.com/wzblog/20231209131238.png">


<link rel="canonical" href="https://wz.anoms.top/2023/12/09/attention_architecture_2/">



<script class="next-config" data-name="page" type="application/json">{"sidebar":"","isHome":false,"isPost":true,"lang":"zh-CN","comments":true,"permalink":"https://wz.anoms.top/2023/12/09/attention_architecture_2/","path":"2023/12/09/attention_architecture_2/","title":"Attention机制与self-attention详解（二）——Transformer架构"}</script>

<script class="next-config" data-name="calendar" type="application/json">""</script>
<title>Attention机制与self-attention详解（二）——Transformer架构 | Warren's Blog</title>
  

  <script src="/js/third-party/analytics/baidu-analytics.js"></script>
  <script async src="https://hm.baidu.com/hm.js?b9a00db803ec11a8bedcc6012afff1b3"></script>





  <noscript>
    <link rel="stylesheet" href="/css/noscript.css">
  </noscript>
</head>

<body itemscope itemtype="http://schema.org/WebPage">
  <div class="headband"></div>

  <main class="main">
    <header class="header" itemscope itemtype="http://schema.org/WPHeader">
      <div class="header-inner"><div class="site-brand-container">
  <div class="site-nav-toggle">
    <div class="toggle" aria-label="切换导航栏" role="button">
        <span class="toggle-line"></span>
        <span class="toggle-line"></span>
        <span class="toggle-line"></span>
    </div>
  </div>

  <div class="site-meta">

    <a href="/" class="brand" rel="start">
      <i class="logo-line"></i>
      <p class="site-title">Warren's Blog</p>
      <i class="logo-line"></i>
    </a>
  </div>

  <div class="site-nav-right">
    <div class="toggle popup-trigger">
        <i class="fa fa-search fa-fw fa-lg"></i>
    </div>
  </div>
</div>



<nav class="site-nav">
  <ul class="main-menu menu"><li class="menu-item menu-item-home"><a href="/" rel="section"><i class="fa fa-home fa-fw"></i>首页</a></li><li class="menu-item menu-item-tags"><a href="/tags/" rel="section"><i class="fa fa-tags fa-fw"></i>标签</a></li><li class="menu-item menu-item-categories"><a href="/categories/" rel="section"><i class="fa fa-th fa-fw"></i>分类</a></li><li class="menu-item menu-item-archives"><a href="/archives/" rel="section"><i class="fa fa-archive fa-fw"></i>归档</a></li><li class="menu-item menu-item-友链"><a href="/%E5%8F%8B%E9%93%BE/" rel="section"><i class="fa fa-user-group fa-fw"></i>友链</a></li><li class="menu-item menu-item-about"><a href="/%E5%85%B3%E4%BA%8E/" rel="section"><i class="fa fa-circle-info fa-fw"></i>关于</a></li>
      <li class="menu-item menu-item-search">
        <a role="button" class="popup-trigger"><i class="fa fa-search fa-fw"></i>搜索
        </a>
      </li>
  </ul>
</nav>



  <div class="search-pop-overlay">
    <div class="popup search-popup"><div class="search-header">
  <span class="search-icon">
    <i class="fa fa-search"></i>
  </span>
  <div class="search-input-container">
    <input autocomplete="off" autocapitalize="off" maxlength="80"
           placeholder="搜索..." spellcheck="false"
           type="search" class="search-input">
  </div>
  <span class="popup-btn-close" role="button">
    <i class="fa fa-times-circle"></i>
  </span>
</div>
<div class="search-result-container no-result">
  <div class="search-result-icon">
    <i class="fa fa-spinner fa-pulse fa-5x"></i>
  </div>
</div>

    </div>
  </div>

</div>
        
  
  <div class="toggle sidebar-toggle" role="button">
    <span class="toggle-line"></span>
    <span class="toggle-line"></span>
    <span class="toggle-line"></span>
  </div>

  <aside class="sidebar">

    <div class="sidebar-inner sidebar-nav-active sidebar-toc-active">
      <ul class="sidebar-nav">
        <li class="sidebar-nav-toc">
          文章目录
        </li>
        <li class="sidebar-nav-overview">
          站点概览
        </li>
      </ul>

      <div class="sidebar-panel-container">
        <!--noindex-->
        <div class="post-toc-wrap sidebar-panel">
            <div class="post-toc animated"><ol class="nav"><li class="nav-item nav-level-2"><a class="nav-link" href="#%E4%B8%80%E3%80%81%E5%BC%95%E8%A8%80%EF%BC%9ASelf-attention%EF%BC%88Seq2seq%EF%BC%89%E7%9A%84%E5%B8%B8%E8%A7%81%E5%BA%94%E7%94%A8"><span class="nav-number">1.</span> <span class="nav-text">一、引言：Self-attention（Seq2seq）的常见应用</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E4%BA%8C%E3%80%81transformer%E6%A8%A1%E5%9E%8B%EF%BC%88Seq2seq%E6%A8%A1%E5%9E%8B%EF%BC%89%E7%9A%84%E7%BB%93%E6%9E%84"><span class="nav-number">2.</span> <span class="nav-text">二、transformer模型（Seq2seq模型）的结构</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#%EF%BC%88%E4%B8%80%EF%BC%89%E7%BB%93%E6%9E%84%EF%BC%9AEncoder-Decoder"><span class="nav-number">2.1.</span> <span class="nav-text">（一）结构：Encoder-Decoder</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#1-Encoder"><span class="nav-number">2.1.1.</span> <span class="nav-text">1. Encoder</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#2-Decoder"><span class="nav-number">2.1.2.</span> <span class="nav-text">2. Decoder</span></a><ol class="nav-child"><li class="nav-item nav-level-5"><a class="nav-link" href="#2-1-autoregressive-decoder-AT"><span class="nav-number">2.1.2.1.</span> <span class="nav-text">2.1 autoregressive decoder (AT)</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#2-2-non-autoregressive-decoder-NAT"><span class="nav-number">2.1.2.2.</span> <span class="nav-text">2.2 non-autoregressive decoder (NAT)</span></a></li></ol></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%EF%BC%88%E4%BA%8C%EF%BC%89Encoder%E4%B8%8EDecoder%E7%9A%84%E8%BF%9E%E6%8E%A5"><span class="nav-number">2.2.</span> <span class="nav-text">（二）Encoder与Decoder的连接</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E4%B8%89%E3%80%81Transformer%E7%9A%84%E8%AE%AD%E7%BB%83"><span class="nav-number">3.</span> <span class="nav-text">三、Transformer的训练</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#%EF%BC%88%E4%B8%80%EF%BC%89%E8%BE%93%E5%87%BA%E7%BB%93%E6%9E%9C%E5%BA%A6%E9%87%8F"><span class="nav-number">3.1.</span> <span class="nav-text">（一）输出结果度量</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%EF%BC%88%E4%BA%8C%EF%BC%89%E6%89%A9%E5%B1%95%E7%9F%A5%E8%AF%86%E7%82%B9"><span class="nav-number">3.2.</span> <span class="nav-text">（二）扩展知识点</span></a></li></ol></li></ol></div>
        </div>
        <!--/noindex-->

        <div class="site-overview-wrap sidebar-panel">
          <div class="site-author site-overview-item animated" itemprop="author" itemscope itemtype="http://schema.org/Person">
    <img class="site-author-image" itemprop="image" alt="Warren Z"
      src="/images/cyclinbox.png">
  <p class="site-author-name" itemprop="name">Warren Z</p>
  <div class="site-description" itemprop="description">小楼一夜听春雨，深巷明朝卖杏花</div>
</div>
<div class="site-state-wrap site-overview-item animated">
  <nav class="site-state">
      <div class="site-state-item site-state-posts">
        <a href="/archives/">
          <span class="site-state-item-count">235</span>
          <span class="site-state-item-name">日志</span>
        </a>
      </div>
      <div class="site-state-item site-state-categories">
          <a href="/categories/">
        <span class="site-state-item-count">4</span>
        <span class="site-state-item-name">分类</span></a>
      </div>
      <div class="site-state-item site-state-tags">
          <a href="/tags/">
        <span class="site-state-item-count">427</span>
        <span class="site-state-item-name">标签</span></a>
      </div>
  </nav>
</div>
  <div class="links-of-author site-overview-item animated">
      <span class="links-of-author-item">
        <a href="https://github.com/cyclinbox" title="GitHub → https:&#x2F;&#x2F;github.com&#x2F;cyclinbox" rel="noopener" target="_blank"><i class="fab fa-github fa-fw"></i>GitHub</a>
      </span>
      <span class="links-of-author-item">
        <a href="mailto:zhangwanyu2000@outlook.com" title="E-Mail → mailto:zhangwanyu2000@outlook.com" rel="noopener" target="_blank"><i class="fa fa-envelope fa-fw"></i>E-Mail</a>
      </span>
      <span class="links-of-author-item">
        <a href="/" title="回主页 → &#x2F;"><i class="fa fa-home fa-fw"></i>回主页</a>
      </span>
  </div>
  <div class="cc-license site-overview-item animated" itemprop="license">
    <a href="https://creativecommons.org/licenses/by-nc-sa/4.0/" class="cc-opacity" rel="noopener" target="_blank"><img src="https://cdnjs.cloudflare.com/ajax/libs/creativecommons-vocabulary/2020.11.3/assets/license_badges/small/by_nc_sa.svg" alt="Creative Commons"></a>
  </div>



        </div>
      </div>
    </div>
  </aside>
  <div class="sidebar-dimmer"></div>


    </header>

    
  <div class="back-to-top" role="button" aria-label="返回顶部">
    <i class="fa fa-arrow-up"></i>
    <span>0%</span>
  </div>
  <div class="reading-progress-bar"></div>

  <a href="https://github.com/cyclinbox" class="github-corner" title="My GitHub" aria-label="My GitHub" rel="noopener" target="_blank"><svg width="80" height="80" viewBox="0 0 250 250" aria-hidden="true"><path d="M0,0 L115,115 L130,115 L142,142 L250,250 L250,0 Z"></path><path d="M128.3,109.0 C113.8,99.7 119.0,89.6 119.0,89.6 C122.0,82.7 120.5,78.6 120.5,78.6 C119.2,72.0 123.4,76.3 123.4,76.3 C127.3,80.9 125.5,87.3 125.5,87.3 C122.9,97.6 130.6,101.9 134.4,103.2" fill="currentColor" style="transform-origin: 130px 106px;" class="octo-arm"></path><path d="M115.0,115.0 C114.9,115.1 118.7,116.5 119.8,115.4 L133.7,101.6 C136.9,99.2 139.9,98.4 142.2,98.6 C133.8,88.0 127.5,74.4 143.8,58.0 C148.5,53.4 154.0,51.2 159.7,51.0 C160.3,49.4 163.2,43.6 171.4,40.1 C171.4,40.1 176.1,42.5 178.8,56.2 C183.1,58.6 187.2,61.8 190.9,65.4 C194.5,69.0 197.7,73.2 200.1,77.6 C213.8,80.2 216.3,84.9 216.3,84.9 C212.7,93.1 206.9,96.0 205.4,96.6 C205.1,102.4 203.0,107.8 198.3,112.5 C181.9,128.9 168.3,122.5 157.7,114.1 C157.9,116.9 156.7,120.9 152.7,124.9 L141.0,136.5 C139.8,137.7 141.6,141.9 141.8,141.8 Z" fill="currentColor" class="octo-body"></path></svg></a>

<noscript>
  <div class="noscript-warning">Theme NexT works best with JavaScript enabled</div>
</noscript>


    <div class="main-inner post posts-expand">


  


<div class="post-block">
  
  

  <article itemscope itemtype="http://schema.org/Article" class="post-content" lang="zh-CN">
    <link itemprop="mainEntityOfPage" href="https://wz.anoms.top/2023/12/09/attention_architecture_2/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/cyclinbox.png">
      <meta itemprop="name" content="Warren Z">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Warren's Blog">
      <meta itemprop="description" content="小楼一夜听春雨，深巷明朝卖杏花">
    </span>

    <span hidden itemprop="post" itemscope itemtype="http://schema.org/CreativeWork">
      <meta itemprop="name" content="Attention机制与self-attention详解（二）——Transformer架构 | Warren's Blog">
      <meta itemprop="description" content="">
    </span>
      <header class="post-header">
        <h1 class="post-title" itemprop="name headline">
          Attention机制与self-attention详解（二）——Transformer架构
        </h1>

        <div class="post-meta-container">
          <div class="post-meta">
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar"></i>
      </span>
      <span class="post-meta-item-text">发表于</span>

      <time title="创建时间：2023-12-09 21:38:09" itemprop="dateCreated datePublished" datetime="2023-12-09T21:38:09+08:00">2023-12-09</time>
    </span>
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar-check"></i>
      </span>
      <span class="post-meta-item-text">更新于</span>
      <time title="修改时间：2023-12-10 23:13:39" itemprop="dateModified" datetime="2023-12-10T23:13:39+08:00">2023-12-10</time>
    </span>
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-folder"></i>
      </span>
      <span class="post-meta-item-text">分类于</span>
        <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
          <a href="/categories/%E8%AE%A1%E7%AE%97%E6%9C%BA/" itemprop="url" rel="index"><span itemprop="name">计算机</span></a>
        </span>
    </span>

  
</div>

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">
        <p>时隔半年，再次复习一下attention机制，并学习一下什么是self-attention。</p>
<span id="more"></span>


<blockquote>
<p>参考： <a target="_blank" rel="noopener" href="https://www.bilibili.com/video/BV1v3411r78R">https://www.bilibili.com/video/BV1v3411r78R</a></p>
</blockquote>
<h2 id="一、引言：Self-attention（Seq2seq）的常见应用"><a href="#一、引言：Self-attention（Seq2seq）的常见应用" class="headerlink" title="一、引言：Self-attention（Seq2seq）的常见应用"></a>一、引言：Self-attention（Seq2seq）的常见应用</h2><ul>
<li>语音识别</li>
<li>文本翻译</li>
<li>语音翻译（直接将一种语言的音频转化为另一种语言的音频，对于一些没有语言的文字，或者一些语言方言的翻译很有用）</li>
</ul>
<p><img src="https://wzblog-1311469384.cos.ap-shanghai.myqcloud.com/wzblog/20231209131238.png" alt="image.png"></p>
<p>上图：使用1500小时的台语（闽南语？）乡土剧视频资料（带有中文字幕）进行训练，训练前不处理背景音乐的noise问题、语音与字母可能对不上的问题等（所谓“硬train一发”），得到的机器学习模型依然表现出了很强的闽南语识别与翻译的能力。</p>
<ul>
<li><p>Seq2seq用于聊天机器人任务（ChatBot）</p>
<ul>
<li>训练集：对话数据。</li>
<li><img src="https://wzblog-1311469384.cos.ap-shanghai.myqcloud.com/wzblog/20231209131916.png" alt="image.png"></li>
<li>许多任务也可以抽象为“Question-Answering（QA）”的模型，例如文本翻译、文章摘要、情感判断等，并使用Transformer训练。<ul>
<li>当然，对于这些特定任务，其实也不一定用QA这种模型，用一些针对特定任务设计的专用模型可能更好（所谓杀鸡焉用牛刀）</li>
</ul>
</li>
<li><img src="https://wzblog-1311469384.cos.ap-shanghai.myqcloud.com/wzblog/20231209132152.png" alt="image.png"></li>
</ul>
</li>
<li><p>seq2seq用于文法剖析（syntactic parsing）</p>
<ul>
<li><img src="https://wzblog-1311469384.cos.ap-shanghai.myqcloud.com/wzblog/20231209132546.png" alt="image.png"></li>
</ul>
</li>
<li><p>seq2seq用于多标签分类问题（multi-label classification）</p>
<ul>
<li>注意与多类别分类问题（multi-class classification）的区别：一个输入对象可能属于多个分类（class），但只对应一个标签（label）</li>
<li><img src="https://wzblog-1311469384.cos.ap-shanghai.myqcloud.com/wzblog/20231209132932.png" alt="image.png"></li>
</ul>
</li>
</ul>
<ul>
<li>seq2seq也可以用于图像中的对象识别（虽然有一种硬解的味道在里面）<ul>
<li><img src="https://wzblog-1311469384.cos.ap-shanghai.myqcloud.com/wzblog/20231209133005.png" alt="image.png"></li>
</ul>
</li>
</ul>
<h2 id="二、transformer模型（Seq2seq模型）的结构"><a href="#二、transformer模型（Seq2seq模型）的结构" class="headerlink" title="二、transformer模型（Seq2seq模型）的结构"></a>二、transformer模型（Seq2seq模型）的结构</h2><h3 id="（一）结构：Encoder-Decoder"><a href="#（一）结构：Encoder-Decoder" class="headerlink" title="（一）结构：Encoder-Decoder"></a>（一）结构：Encoder-Decoder</h3><p><img src="https://wzblog-1311469384.cos.ap-shanghai.myqcloud.com/wzblog/20231209133108.png" alt="image.png"></p>
<p>上图是最早提出的Seq2seq模型，用于文本翻译。</p>
<p><img src="https://wzblog-1311469384.cos.ap-shanghai.myqcloud.com/wzblog/20231209133246.png" alt="image.png"></p>
<p>改进后的seq2seq模型，即Transformer</p>
<h4 id="1-Encoder"><a href="#1-Encoder" class="headerlink" title="1. Encoder"></a>1. Encoder</h4><p>结构图如下：</p>
<p><img src="https://wzblog-1311469384.cos.ap-shanghai.myqcloud.com/wzblog/20231209133352.png" alt="image.png"></p>
<p>在encoder内部，是self-attention结构。</p>
<p><img src="https://wzblog-1311469384.cos.ap-shanghai.myqcloud.com/wzblog/20231209133536.png" alt="image.png"></p>
<p>更具体地来看，一个encoder包含了多个block，每个block都是self-attention和全连接神经网络（Fully-connected netwok，FC）的叠加。</p>
<p>下面看看每个block内部的情况：</p>
<p><img src="https://wzblog-1311469384.cos.ap-shanghai.myqcloud.com/wzblog/20231209134025.png" alt="image.png"></p>
<p>如上图，对于Transformer模型的self-attention层，其采用了一种residual network的机制，其输出等于self-attention的输出与输入的线性加和。此后经过一个layer norm层，输出经过归一化的向量。</p>
<p><img src="https://wzblog-1311469384.cos.ap-shanghai.myqcloud.com/wzblog/20231209134443.png" alt="image.png"></p>
<p>在self-attention层之后，还需要经过FC层的处理。和self-attention层类似，FC层处理后也需要经过residual network的加和处理和norm layer的归一化。</p>
<p><img src="https://wzblog-1311469384.cos.ap-shanghai.myqcloud.com/wzblog/20231209134649.png" alt="image.png"></p>
<h4 id="2-Decoder"><a href="#2-Decoder" class="headerlink" title="2. Decoder"></a>2. Decoder</h4><h5 id="2-1-autoregressive-decoder-AT"><a href="#2-1-autoregressive-decoder-AT" class="headerlink" title="2.1 autoregressive decoder (AT)"></a>2.1 autoregressive decoder (AT)</h5><p>最常用的decoder叫做autoregressive decoder</p>
<p><img src="https://wzblog-1311469384.cos.ap-shanghai.myqcloud.com/wzblog/20231209232823.png" alt="image.png"></p>
<p>如上图，autoregressive的输入是一个特殊标识符“BEGIN”。（当然，autoregressive decoder也接受encoder的输入，但这一块内容将在后文中讲述）</p>
<p>当一个标识符输入时，autoregressive会输出一个长度为V的向量（其中V代表token词表的大小。以中文文本处理模型为例，token词表是所有汉字，那么V就是所有的汉字数量），这个输出向量的每个元素代表对应token出现在当前位置上的概率。</p>
<p><img src="https://wzblog-1311469384.cos.ap-shanghai.myqcloud.com/wzblog/20231209233212.png" alt="image.png"></p>
<p>autoregressive会依次产生输出向量，并且第一位的输出会作为第二位的输入进行处理，从而顺序地预测出下一个将要出现的词的概率（可以回想一下，chatGPT在对话的时候，输出内容是一句话一句话往外蹦出的，因为先输出的文字会再次作为输入以用于预测下一句话的输出）</p>
<p><img src="https://wzblog-1311469384.cos.ap-shanghai.myqcloud.com/wzblog/20231209234236.png" alt="image.png"></p>
<p>上图为transformer的结构，其中左边为encoder部分，右边为decoder部分。如果我们将decoder部分中间的那个block（图中绿色阴影部分）删除，则decoder与encoder的结构完全一致。也就是说，decoder其实和encoder很像，除了decoder多了中间那一部分的结构。</p>
<p>另外注意一下，decoder的第一个block中，使用的是masked的多头注意力机制，这与encoder也不一样。</p>
<p><img src="https://wzblog-1311469384.cos.ap-shanghai.myqcloud.com/wzblog/20231209234436.png" alt="image.png"></p>
<p>所谓“masked”，其具体内容如下图所示，即预测当前位置的输出时，只使用当前位置之前的输入信息，而不考虑这个位置以后的信息。例如，预测 $b^2$ 时，只使用 $[a^1,a^2]$ 的信息；预测 $b^3$ 时，只使用 $[a^1,a^2,a^3]$ 的信息。</p>
<p><img src="https://wzblog-1311469384.cos.ap-shanghai.myqcloud.com/wzblog/20231209234529.png" alt="image.png"></p>
<p>这么做的原因很符合直觉。在transformer模型中，encoder要做的是处理所有输入，因此使用的是没有masked的self-attention；而decoder要做的是按顺序的输出信息，后输出的内容要建立在先前输出的内容的基础上（就像人类思考和说话那样），因此需要这样一个mask的机制。</p>
<p><img src="https://wzblog-1311469384.cos.ap-shanghai.myqcloud.com/wzblog/20231209235154.png" alt="image.png"></p>
<p>另一个问题是decoder如何决定停下。例如，decoder的前四个输出是“机器学习”，如果没有停止机制，则decoder可能将第四个输出“习”作为输入，然后预测出第五个输出“惯”（假定这个decoder在训练中学习过“习惯”这个词）。</p>
<p>为了防止这种情况产生，我们还需要加入一个特殊的符号“END”，用来表示停止输出，这样，在输出“机器学习”四个字以后，继续输出得到“END”符号，那么此时的模型就知道输出结束，从而停下。</p>
<p><img src="https://wzblog-1311469384.cos.ap-shanghai.myqcloud.com/wzblog/20231209235637.png" alt="image.png"></p>
<h5 id="2-2-non-autoregressive-decoder-NAT"><a href="#2-2-non-autoregressive-decoder-NAT" class="headerlink" title="2.2 non-autoregressive decoder (NAT)"></a>2.2 non-autoregressive decoder (NAT)</h5><p><img src="https://wzblog-1311469384.cos.ap-shanghai.myqcloud.com/wzblog/20231209235809.png" alt="image.png"></p>
<p>二者的区别如上图。autoregressive decoder只接受一个begin，后续的的输入都来自前一个输出，而non-autogressive decoder可以同时接受多个begin，并同时产生一整句话的输出。</p>
<ul>
<li>如何决定NAT解码器的输出长度？<ul>
<li>使用另一个预测器获得输出长度</li>
<li>或者输出一个很长的序列，然后忽略END之后的标记</li>
</ul>
</li>
<li>优点：NAT可以并行处理加速输出，输出长度可控</li>
<li>缺点：NAT的表现并不如AT，其中的问题有很多，例如multi-modality问题</li>
</ul>
<h3 id="（二）Encoder与Decoder的连接"><a href="#（二）Encoder与Decoder的连接" class="headerlink" title="（二）Encoder与Decoder的连接"></a>（二）Encoder与Decoder的连接</h3><p><img src="https://wzblog-1311469384.cos.ap-shanghai.myqcloud.com/wzblog/20231210000746.png" alt="image.png"></p>
<p>Encoder与Decoder的连接使用了一种叫做cross-attention的机制，如上图所示。这张图就是之前那张transformer结构图中，绿色阴影部分的block内部的结构。</p>
<ul>
<li>注意到，在上图中，encoder部分对输入进行处理，得到一组向量 $[a^1,a^2,a^3,…]$ ，这一组向量再通过数学上的处理得到 $K,V$ 矩阵（即上图的 $[k^1,k^2,k^3,…]$ 和 $[v^1,v^2,v^3,…]$  ）。</li>
<li>与此同时，decoder部分也会进行处理，从第一个token即“BEGIN”开始，通过数学上的处理得到 $Q$ 矩阵（即上图的 $[q,…]$）。</li>
<li>随后就是矩阵乘法的操作即 $\text{softmax}(\frac{Q\times K^T}{\sqrt{d_k}})V$ ，得到输出矩阵 $V$ （即上图中的 $[v,…]$ ）</li>
<li>输出还会经过全连接神经网络（FC）的处理，从而进入下一层。</li>
</ul>
<p><img src="https://wzblog-1311469384.cos.ap-shanghai.myqcloud.com/wzblog/20231210001954.png" alt="image.png"></p>
<p>cross-attention机制最早在一项语音翻译模型的工作中被提出，此时还没有Transformer的架构。如上图，展示了cross-attention的attention score，其中横坐标是输入的音频信号的frame位置，纵坐标是输出的字母的位置，图中方块颜色的深浅代表了对应音频信号frame获得的注意力大小，颜色越深注意力越强。。</p>
<ul>
<li>输出从上到下进行，每生成一个字母，这个模型都会将当前字母和输入的音频信号进行处理，并计算最有可能出现在下一个位置上的字母</li>
<li>而图中的热图就代表了每输出一个字母时，音频信号中的哪一部分获得了最大的注意。</li>
<li>这就是为什么随着输出的进行，注意力最集中的位置也在随音频frame的进行而向右移动（但是这种移动不是完全向右不带回头的）</li>
</ul>
<h2 id="三、Transformer的训练"><a href="#三、Transformer的训练" class="headerlink" title="三、Transformer的训练"></a>三、Transformer的训练</h2><h3 id="（一）输出结果度量"><a href="#（一）输出结果度量" class="headerlink" title="（一）输出结果度量"></a>（一）输出结果度量</h3><p><img src="https://wzblog-1311469384.cos.ap-shanghai.myqcloud.com/wzblog/20231210003140.png" alt="image.png"></p>
<p>transformer使用交叉熵（cross-entropy）对输出结果进行度量，如上图所示，所谓交叉熵度量的是模型在一个位置的输出与这个位置上的真实输出（ground-truth）之间的差异性。训练目标是最小化交叉熵。</p>
<p><img src="https://wzblog-1311469384.cos.ap-shanghai.myqcloud.com/wzblog/20231210003505.png" alt="image.png"></p>
<p>对于每个位置上的输出，都会计算一次交叉熵。</p>
<p><img src="https://wzblog-1311469384.cos.ap-shanghai.myqcloud.com/wzblog/20231210003545.png" alt="image.png"></p>
<p>在训练过程中还使用了一种策略，叫做“teacher forcing”，就是在训练时，不论上一个位置上的实际输出是什么，在预测下一个位置上的输出时都使用ground truth的字符作为输入。</p>
<h3 id="（二）扩展知识点"><a href="#（二）扩展知识点" class="headerlink" title="（二）扩展知识点"></a>（二）扩展知识点</h3><p><strong>复制机制（copy-mechanism）</strong>：模型的输出不一定需要自己生成，也可以直接从输入的内容中复制一小部分。如下图，在聊天机器人任务中，输出内容的部分词汇来自输入的情况很常见；此外，在文章摘要任务中，输出内容来自输入的文章的主旨段落，因此也需要这种机制。</p>
<p><img src="https://wzblog-1311469384.cos.ap-shanghai.myqcloud.com/wzblog/20231210003903.png" alt="image.png"></p>
<p><img src="https://wzblog-1311469384.cos.ap-shanghai.myqcloud.com/wzblog/20231210004139.png" alt="image.png"></p>
<p>想要了解更多，可以查看上述论文。</p>
<p><strong>引导学习（guided attention）</strong> ：硬train一发可能出问题，例如模型忽略了输入中的一些文字，导致输出内容不全（例如文本翻译中一些句子被忽略；语音合成中部分文字被忽略，如下图）。一种方法是通过人工的方法，告诉机器必须采用我们所指定的顺序和方式处理输入内容，从而保证所有的输入都会被用到。一些关键词：Monotonic Attention, Location-aware attention</p>
<p><img src="https://wzblog-1311469384.cos.ap-shanghai.myqcloud.com/wzblog/20231210005034.png" alt="image.png"></p>
<p><strong>集束搜索（Beam Search）</strong> ：如下图，decoder在产生每一步的输出时，会采用一种贪心算法选择当前最有可能的那个输出（红色路径）。然而，全局最优的输出可能是通过绿色的那条搜索路径获得的（但绿色路径在某些位置上不是局部最优）。要想获得全局最优，可以使用集束搜索的策略。</p>
<p><img src="https://wzblog-1311469384.cos.ap-shanghai.myqcloud.com/wzblog/20231210005433.png" alt="image.png"></p>
<p>集束搜索也不一定在所有时候都好。因为这种策略类似于在模型中引入了一些随机性，这种随机性并不一定是好的。当进行语音识别任务时，当然是越精准越好，此时不应该有随机性的成分在里面；而在进行文本续写、文章创作、语音合成时，引入一定的随机性反而会使生成的结果富含创造性。</p>
<p><img src="https://wzblog-1311469384.cos.ap-shanghai.myqcloud.com/wzblog/20231210005808.png" alt="image.png"></p>
<p>扩展：对输出结果的衡量标准能否进行优化？</p>
<ul>
<li>如下图，左边是使用交叉熵衡量输出结果的准确性，相当于每个词各自计算相关性，然而这样有时候并不太好。</li>
<li>另一种方法是将输出的整个句子和ground-truth进行相关性的比较，如右图所示，可以使用一种叫做BLEU score的方法进行。</li>
<li>然而后者的计算量很大，模型也更复杂，所以不是很推荐</li>
<li>如果真的要使用BLEU score，或许可以配合上reinforcement learning的策略，“硬train一发”，就可以得到很好的结果</li>
</ul>
<p><img src="https://wzblog-1311469384.cos.ap-shanghai.myqcloud.com/wzblog/20231210010238.png" alt="image.png"></p>
<p><strong>计划采样（Scheduled sampling）</strong>： 在训练过程中引入噪音（例如，随机替换输入中的一些字符），以增强模型的稳健性。</p>
<p><img src="https://wzblog-1311469384.cos.ap-shanghai.myqcloud.com/wzblog/20231210010740.png" alt="image.png"></p>

    </div>

    
    
    
      


    <footer class="post-footer">
          

<div class="post-copyright">
<ul>
  <li class="post-copyright-author">
      <strong>本文作者： </strong>Warren Z
  </li>
  <li class="post-copyright-link">
      <strong>本文链接：</strong>
      <a href="https://wz.anoms.top/2023/12/09/attention_architecture_2/" title="Attention机制与self-attention详解（二）——Transformer架构">https://wz.anoms.top/2023/12/09/attention_architecture_2/</a>
  </li>
  <li class="post-copyright-license">
    <strong>版权声明： </strong>本博客所有文章除特别声明外，均采用 <a href="https://creativecommons.org/licenses/by-nc-sa/4.0/" rel="noopener" target="_blank"><i class="fab fa-fw fa-creative-commons"></i>BY-NC-SA</a> 许可协议。转载请注明出处！
  </li>
</ul>
</div>

          <div class="post-tags">
              <a href="/tags/%E8%AE%A1%E7%AE%97%E6%9C%BA%E6%8A%80%E6%9C%AF%E6%9D%82%E8%B0%88/" rel="tag"><i class="fa fa-tag"></i> 计算机技术杂谈</a>
              <a href="/tags/Attention%E6%9C%BA%E5%88%B6/" rel="tag"><i class="fa fa-tag"></i> Attention机制</a>
              <a href="/tags/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/" rel="tag"><i class="fa fa-tag"></i> 机器学习</a>
              <a href="/tags/Transformer/" rel="tag"><i class="fa fa-tag"></i> Transformer</a>
          </div>

        

          <div class="post-nav">
            <div class="post-nav-item">
                <a href="/2023/12/03/attention_architecture_1/" rel="prev" title="Attention机制与self-attention详解（一）——基础知识">
                  <i class="fa fa-chevron-left"></i> Attention机制与self-attention详解（一）——基础知识
                </a>
            </div>
            <div class="post-nav-item">
                <a href="/2023/12/10/vscode%E9%85%8D%E7%BD%AE%E8%BF%9C%E7%A8%8B%E6%9C%8D%E5%8A%A1%E5%99%A8/" rel="next" title="VS Code配置远程服务器插件">
                  VS Code配置远程服务器插件 <i class="fa fa-chevron-right"></i>
                </a>
            </div>
          </div>
    </footer>
  </article>
</div>






    
  
  <div class="comments giscus-container">
  </div>
  
  
</div>
  </main>

  <footer class="footer">
    <div class="footer-inner">

  <div class="beian"><a href="https://beian.miit.gov.cn/" rel="noopener" target="_blank">津ICP备20004656号 </a>
  </div>

<div class="copyright">
  &copy; 2020 – 
  <span itemprop="copyrightYear">2025</span>
  <span class="with-love">
    <i class="fa fa-heart"></i>
  </span>
  <span class="author" itemprop="copyrightHolder">Warren Z</span>
</div>
  <div class="powered-by">由 <a href="https://hexo.io/" rel="noopener" target="_blank">Hexo</a> & <a href="https://theme-next.js.org/" rel="noopener" target="_blank">NexT.Gemini</a> 强力驱动
  </div>

    </div>
  </footer>

  
  <script src="https://cdnjs.cloudflare.com/ajax/libs/animejs/3.2.1/anime.min.js" integrity="sha256-XL2inqUJaslATFnHdJOi9GfQ60on8Wx1C2H8DYiN1xY=" crossorigin="anonymous"></script>
<script src="/js/comments.js"></script><script src="/js/utils.js"></script><script src="/js/next-boot.js"></script>

  
<script src="https://cdnjs.cloudflare.com/ajax/libs/hexo-generator-searchdb/1.4.0/search.js" integrity="sha256-vXZMYLEqsROAXkEw93GGIvaB2ab+QW6w3+1ahD9nXXA=" crossorigin="anonymous"></script>
<script src="/js/third-party/search/local-search.js"></script>




  <script src="/js/third-party/pace.js"></script>

  




  

  <script class="next-config" data-name="enableMath" type="application/json">true</script><script class="next-config" data-name="mathjax" type="application/json">{"enable":true,"tags":"ams","js":{"url":"https://cdnjs.cloudflare.com/ajax/libs/mathjax/3.2.2/es5/tex-mml-chtml.js","integrity":"sha256-MASABpB4tYktI2Oitl4t+78w/lyA+D7b/s9GEP0JOGI="}}</script>
<script src="/js/third-party/math/mathjax.js"></script>


<script class="next-config" data-name="giscus" type="application/json">{"enable":true,"repo":"cyclinbox/cyclinbox.github.io","repo_id":"MDEwOlJlcG9zaXRvcnk0MDQ5ODMzNzY=","category":"General","category_id":"DIC_kwDOGCOOUM4CWZSA","mapping":"pathname","reactions_enabled":1,"emit_metadata":1,"theme":"preferred-color-scheme","lang":"zh-CN","input_position":"top"}</script>

<script>
document.addEventListener('page:loaded', () => {
  if (!CONFIG.page.comments) return;

  NexT.utils.loadComments('.giscus-container')
    .then(() => NexT.utils.getScript('https://giscus.app/client.js', {
      attributes: {
        async                   : true,
        crossOrigin             : 'anonymous',
        'data-repo'             : CONFIG.giscus.repo,
        'data-repo-id'          : CONFIG.giscus.repo_id,
        'data-category'         : CONFIG.giscus.category,
        'data-category-id'      : CONFIG.giscus.category_id,
        'data-mapping'          : CONFIG.giscus.mapping,
        'data-reactions-enabled': CONFIG.giscus.reactions_enabled,
        'data-emit-metadata'    : CONFIG.giscus.emit_metadata,
        'data-theme'            : CONFIG.giscus.theme,
        'data-lang'             : CONFIG.giscus.lang,
        'data-input-position'   : CONFIG.giscus.input_position,
        'data-loading'          : CONFIG.giscus.loading
      },
      parentNode: document.querySelector('.giscus-container')
    }));
});
</script>

<!-- hexo injector body_end start -->
    <link rel="stylesheet" crossorigin href="https://g.alicdn.com/aliyun-documentation/web-chatbot-ui/0.0.24/index.css" />
    <script type="module" crossorigin src="https://g.alicdn.com/aliyun-documentation/web-chatbot-ui/0.0.24/index.js"></script>
    <script>
      window.CHATBOT_CONFIG = {
        //endpoint: "/chat", // 可以替换为 https://{your-fc-http-trigger-domain}/chat
        endpoint: "https://webchat-bot-pbz-ojggjrcplw.cn-hangzhou.fcapp.run/chat",  
        displayByDefault: false, // 默认不显示 AI 助手对话框
        title: 'Warren的桌宠', // 自定义 AI 助手标题
        draggable: true, // 是否开启拖拽
        aiChatOptions: { // 自定义取值参考：https://docs.nlkit.com/nlux/reference/ui/ai-chat#conversation-options
          conversationOptions: { // 自定义取值参考：https://docs.nlkit.com/nlux/reference/ui/ai-chat#conversation-options
            conversationStarters: [
              {prompt: '介绍一下Warren blog？'},
              {prompt: '博主对人生和社会的思考有哪些？'},
              {prompt: '在这个博客中，有哪些实用的计算机技术?'},
            ]
          },
          displayOptions: { // 自定义取值参考：https://docs.nlkit.com/nlux/reference/ui/ai-chat#display-options
            height: 600,
            // width: 400,
          },
          personaOptions: { // 自定义取值参考：https://docs.nlkit.com/nlux/reference/ui/ai-chat#chat-personas
            assistant: {
              name: 'Warren blog对话助手（powered by 通义千问）',
              // AI 助手的图标
              avatar: 'https://pic1.imgdb.cn/item/67b1ca92d0e0a243d4ffd69b.jpg',
		//https://img.alicdn.com/imgextra/i2/O1CN01Pda9nq1YDV0mnZ31H_!!6000000003025-54-tps-120-120.apng',
              tagline: '您可以尝试点击下方的快捷入口开启体验！',
            }
          },
          messageOptions: { // 自定义取值参考：https://docs.nlkit.com/nlux/reference/ui/ai-chat#message-options
            waitTimeBeforeStreamCompletion: 'never'
          }
        },
        dataProcessor: {
          /**
          * 在向后端大模型应用发起请求前改写 Prompt。
          * 比如可以用于总结网页场景，在发送前将网页内容包含在内，同时避免在前端显示这些内容。
          * @param {string} prompt - 用户输入的 Prompt
          * @param {string}  - 改写后的 Prompt
          */
          rewritePrompt(prompt) {
            return prompt;
          }
        }
      };
    </script>
    <style>
      :root {
        /* webchat 工具栏的颜色 */
        --webchat-toolbar-background-color: #1464E4;
        /* webchat 工具栏文字和按钮的颜色 */
        --webchat-toolbar-text-color: #FFF;
      }
      /* webchat 对话框如果被遮挡，可以尝试通过 z-index、bottom、right 等设置 来调整*/
      .webchat-container {
        z-index: 100;
        bottom: 10px;
        right: 10px;
      }
      /* webchat 的唤起按钮如果被遮挡，可以尝试通过 z-index、bottom、right 等设置 来调整。也可以通过 CSS 进一步定制唤起按钮的形状、大小等。 */
      .webchat-bubble-tip {
        z-index: 99;
        bottom: 20px;
        right: 20px;
      }
    </style> 
<!-- hexo injector body_end end --></body>
</html>
